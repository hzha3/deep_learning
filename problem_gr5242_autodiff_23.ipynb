{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0679FMjuAQV"
   },
   "source": [
    "# **GR5242 HW03 Problem 1: Automatic Differentiation**\n",
    "\n",
    "**Instructions**: This problem is an individual assignment -- you are to complete this problem on your own, without conferring with your classmates.  You should submit a completed and published notebook to Courseworks; no other files will be accepted.\n",
    "\n",
    "## Description:\n",
    "\n",
    "This homework adds more detail to the autodiff lecture in class. There are seven questions (30 points) in total, which include coding and written questions. You can only modify the codes and text within \\### YOUR CODE HERE ### and/or \\### YOUR ANSWER HERE ###.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XSFRWzPgRW1S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sympy as sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B6xtlRiMZ4z"
   },
   "source": [
    "## Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkLs4MRfMeBK"
   },
   "source": [
    "**Different ways of differentiation:** In this problem we consider a particular function (which we call it an `op` in the code block, short for operation) $f:\\mathbb{R} \\to \\mathbb{R}: x \\mapsto 4x(1-x)$ and it's compositions with itself. Let us define,\n",
    "$$ g_n: \\underbrace{f \\circ \\dots \\circ f}_{n \\text{ times}}: x \\mapsto f^{(n)}(x) $$\n",
    "where $f^{(n)}(x) = f(f^{(n-1)}(x))$ is the $n$ times composited function $f$ with itself. The goal of this problem is to explore various ways of differentiation, i.e. $\\frac{d}{dx}g_{n}(x)$ which are listed below:\n",
    "- Numerical differentiation\n",
    "- Symbolic differentiation\n",
    "- Autodiff in forward or reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yeN0UEVXMgj5"
   },
   "outputs": [],
   "source": [
    "def op(inp):\n",
    "    return 4 * inp * ( 1 - inp )\n",
    "\n",
    "def operation(inp, n = 1):\n",
    "    temp = inp\n",
    "    for i in range(n):\n",
    "        temp = op(temp)\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nHw-ZVUvLUE"
   },
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "Based on the definition of derivative at a particular point $x_0$,\n",
    "\n",
    "$$ \\frac{d}{dx}g_n(x_0) = \\lim_{h \\to 0} \\frac{g_n(x_0+h) - g_n(x_0)}{h} =: \\lim_{h \\to 0} D_h g_n(x_0) $$\n",
    "\n",
    "The formula above suggests a simple way of **approximating** the derivative by taking $D_{h}g_n(x_0)$ for a particular choice of $h$ as your approximate derivative. This is also known as the [finite difference](https://en.wikipedia.org/wiki/Numerical_differentiation) method. Note that this approach only requires evaluation of your function around the point you are trying to take the derivative at so it's computationally efficient but the caveat is that choosing a proper $h$ to obtain good enough approximations is generally hard (specially when the function is multivariate). However, in our case we have some structure over the function we are trying to take derivative of.\n",
    "\n",
    "**Question 1 (3 points)**: Use a finite difference with tolerance 1e-12 to approximate the derivative of $g_3(x)$ at $x=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u2mqa0N1xgD_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64.00568963861407"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tol = 1e-12\n",
    "### Your Code Here ###\n",
    "(operation(inp=1+tol, n=3)-operation(inp=1, n=3))/ tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXTYdS8ZDTZt"
   },
   "source": [
    "The approximation will be bad at certain points $x$, and becomes less stable as $n$ becomes larger for a fixed $h$ (or as the dimensionality of the function grows).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umbqG5P5a5pi"
   },
   "source": [
    "## Calculus and (manual/symbolic) differentiation\n",
    "\n",
    "Notice that $f$ is a polynomial and that composition of polynomials yields also a polynomial. Therefore, $g_n$ is a polynomial with degree $2^n$ (try to argue this for yourself using induction); we can compute the derivative using  calculus.\n",
    "\n",
    "Instead of computing the derivative by hand we use the help of an auxillary package `sympy` and try to compute the derivative. This package uses symbolic variables and traces operations such as add, multiplication, division, etc., applied onto these variables and computes the derivative using chain rule.\n",
    "\n",
    "It is not difficult to (manually) derive a closed form expression for $g_n$ using the recursive formula\n",
    "$$ g_n(x) = 4 g_{n-1}(x) (1-g_{n-1}(x)), \\quad g_{1}(x) = 4x(1-x).$$  \n",
    "The following block of code prints this closed form expression in terms of a symbolic variable `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Kuijq55eM_YJ"
   },
   "outputs": [],
   "source": [
    "X = sym.Symbol('X') # Create a symbolic variable\n",
    "sym.init_printing(use_unicode=False, wrap_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMebchBYdrbO"
   },
   "source": [
    "Now that we have the closed form expression, we may use the chain rule to express the derivative of $g_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "WXjaKBNOdgRQ",
    "outputId": "53875ec3-a3f9-427d-e77e-f860749b7fe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zha\\.conda\\envs\\tf2\\lib\\site-packages\\IPython\\lib\\latextools.py:126: MatplotlibDeprecationWarning: \n",
      "The to_png function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use mathtext.math_to_image instead.\n",
      "  mt.to_png(f, s, fontsize=12, dpi=dpi, color=color)\n",
      "C:\\Users\\zha\\.conda\\envs\\tf2\\lib\\site-packages\\IPython\\lib\\latextools.py:126: MatplotlibDeprecationWarning: \n",
      "The to_rgba function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use mathtext.math_to_image instead.\n",
      "  mt.to_png(f, s, fontsize=12, dpi=dpi, color=color)\n",
      "C:\\Users\\zha\\.conda\\envs\\tf2\\lib\\site-packages\\IPython\\lib\\latextools.py:126: MatplotlibDeprecationWarning: \n",
      "The to_mask function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use mathtext.math_to_image instead.\n",
      "  mt.to_png(f, s, fontsize=12, dpi=dpi, color=color)\n",
      "C:\\Users\\zha\\.conda\\envs\\tf2\\lib\\site-packages\\IPython\\lib\\latextools.py:126: MatplotlibDeprecationWarning: \n",
      "The MathtextBackendBitmap class was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use mathtext.math_to_image instead.\n",
      "  mt.to_png(f, s, fontsize=12, dpi=dpi, color=color)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 64 X \\left(1 - X\\right) \\left(8 X - 4\\right) \\left(- 16 X \\left(1 - X\\right) \\left(- 4 X \\left(1 - X\\right) + 1\\right) + 1\\right) + 64 X \\left(1 - X\\right) \\left(- 4 X \\left(1 - X\\right) + 1\\right) \\left(- 16 X \\left(1 - X\\right) \\left(8 X - 4\\right) + 16 X \\left(- 4 X \\left(1 - X\\right) + 1\\right) - 16 \\cdot \\left(1 - X\\right) \\left(- 4 X \\left(1 - X\\right) + 1\\right)\\right) - 64 X \\left(- 4 X \\left(1 - X\\right) + 1\\right) \\left(- 16 X \\left(1 - X\\right) \\left(- 4 X \\left(1 - X\\right) + 1\\right) + 1\\right) + 64 \\cdot \\left(1 - X\\right) \\left(- 4 X \\left(1 - X\\right) + 1\\right) \\left(- 16 X \\left(1 - X\\right) \\left(- 4 X \\left(1 - X\\right) + 1\\right) + 1\\right)$"
      ],
      "text/plain": [
       "64*X*(1 - X)*(8*X - 4)*(-16*X*(1 - X)*(-4*X*(1 - X) + 1) + 1) + 64*X*(1 - X)*(\n",
       "-4*X*(1 - X) + 1)*(-16*X*(1 - X)*(8*X - 4) + 16*X*(-4*X*(1 - X) + 1) - 16*(1 -\n",
       " X)*(-4*X*(1 - X) + 1)) - 64*X*(-4*X*(1 - X) + 1)*(-16*X*(1 - X)*(-4*X*(1 - X)\n",
       " + 1) + 1) + 64*(1 - X)*(-4*X*(1 - X) + 1)*(-16*X*(1 - X)*(-4*X*(1 - X) + 1) +\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = operation(X, 3)\n",
    "dydx_symbolic = sym.diff(Y, X)\n",
    "dydx_symbolic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89wwtEqZ5NP1"
   },
   "source": [
    "**Question 2 (3 points):** Compute the exact derivative of $g_n(x)$ at $x=1$ by evaluating the symbolic expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HB3DU8l29quU"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD8AAAASCAYAAADsbQY3AAACwUlEQVR4nO3XX4hXVRAH8M/PNND8kxD7pMgqKoE9JGlppGa6ij4p9WIIRQmiYGWBoGYrEeSD1q4khJqKvogoCBYpLcFSKT0EPURlrCwSSqmoaK1Wag9zrvvzunf//Pb2tA0cvpeZMzNn7pkzZ06lsbHRQKVB/5HdZ3AY53Ez4Qks6oXuctxJ49UafI/BJziXfLfjQ4zOTxxcg/GeaCPexUUcE4E/gscxB591ozsW23Edw2vwPQHfoA5H8ROm4zUsxNO4lE0uO/gXROBfYCmu5eRDutGtYE9a3BG8VYP/HSLwNeInZrQNb+A9rMyYZab9IGzBn1jm/sDh727012AuXsYfNfgfjwaR5h/lZO8km8vxUPWCy6KZqBdpfRmLsU6k3IwedB/F+2hCa43+5yY8gds52TV8jWF4KmOWmfbTEv6G7/BYTt6K53Ehxx+M/TiL9f3wPznh6QL5LyIzJqGFcne+LuFKDMU8jMAUHMcsHOpCb5Mohi+hox/+RyW8WiDP+A9njHzw7Tqvmd6MA1W6DySsiB1uEVX7ByzBr5jt3iMwXez2VpzsKbp+UiXhnYyRT/s23OiDwXNV35cTnsH3uXkdYvdfEQGf1Jnup/F2H3wWUbazowrkI3Pz7gv+uX44/znhlQJ59nOGJhwuzh/FP3xnGk14vZf+JxXIJya8WxPKLHit+Cc5eRB/5eRTErYnvIndBbamijrwlQiqN0fiy4QN4jhXV/wRosHpwKmMWWbwF3EQL4oitrFKNh8LRMp9nngditvXRhH8PuzqQj5BNExtOnuHNnHNNWC1e5uczeJ+/1hVD1F2h7cWT2KDqO7fYpwoeLewQvGx6Au1JLv1OjMJVon2tlkc4R/Tep4V6b6h2kjZD5vfk7MPRJ+edW2fisdOV1ddmdSGJ7A3reNNkSXN4pa5VD258v+TdoDSgA7+X4Yrnfh/1LzFAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle -64.0$"
      ],
      "text/plain": [
       "-64.0000000000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hint: read about subs/evalf here: https://docs.sympy.org/latest/modules/numeric-computation.html\n",
    "### Your Code Here ###\n",
    "from sympy import *\n",
    "from sympy.abc import x\n",
    "dydx_symbolic.evalf(subs={X: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsxmwxI-q3-P"
   },
   "source": [
    "As it is evident the closed form expression for `dydx_symbolic` is unwieldy. We can make it more efficient by expanding and collecting all the terms. The goal is to represent the derivative as a natural polynomial. Yet this is another way of computing the derivative.\n",
    "\n",
    "**Question 3 (3 points):** Using the `sympy` documentation [page](https://docs.sympy.org/latest/tutorials/intro-tutorial/simplification.html) expand/simplify the closed form expression of $g_n(x)$ and print its derivative as a symbolic expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsvD8llQTLTd",
    "outputId": "ff1af873-7498-4d74-b09c-d96f715f3cbe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle - 131072 X^{7} + 458752 X^{6} - 638976 X^{5} + 450560 X^{4} - 168960 X^{3} + 32256 X^{2} - 2688 X + 64$"
      ],
      "text/plain": [
       "          7           6           5           4           3          2        \n",
       "- 131072*X  + 458752*X  - 638976*X  + 450560*X  - 168960*X  + 32256*X  - 2688*\n",
       "\n",
       "      \n",
       "X + 64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your Code Here ###\n",
    "polynomial = dydx_symbolic\n",
    "dydx_symbolic_simplified = expand(polynomial)\n",
    "dydx_symbolic_simplified # This should express a polynomial of degree 2^n - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jtG76lBAaVF"
   },
   "source": [
    "Now that we have a compact closed form for the derivative we can evaluate it efficiently at various values of $x$.\n",
    "\n",
    "**Question 4 (2 points):** Evaluate `dydx_symbolic_simplified` at $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k2CyEQcHp1HN"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAVCAYAAACpO2fTAAAEd0lEQVR4nO3aWYxUVRAG4G8AjSMgGA0xcUUUYgKJ4q4BFWFI9MX1RcUNjUQjrtEoqOOuD4BA1Ljj8qAxYzSIQSIxQRGDcXswKDo4UQMRRURURMHxoe51mjt9oad7unvQ+ZNOpU/1OVW3uk6dqjq3obm5WS96US765Iw/h7XoX0NdetFzcSTaMTnLKOZAR+ECPIDfqqTQGLRgDTYndBFOK2HuJPEw7bisDNn74WmsTmS34SHsWcZaPQ3VsuuHeBX3YEAho5gD3Ydf8GjpencJ07EEY7EQMzBf/IEn72Du/piLX8uUPUwY4xIsxyyswjVYhr3KXLcnoNp2vR/7YGrhYL/Mj4ZjPJ7EptL07hLOxd14C2dhY4a/y3bmNuAZrMMruLEM+Y9giDDC3ILxmbgO92JKGevWG7Ww63J8jivwILbSOQJdmiz4Uum6l4w+ieDfcZ7ODwl/bWf+VIwT0aOco/VgNIkj6+EM745kzUl2vryvlnZ9EQeIIPOv8EKMF571/g4WKgcnYCjewHqcjpvF8XH8DuYeJnKy2SJMl4NxCV2EvzO8jViK3XFcmevXC7W069KETkgHCo+w/jgcK1QneT46od/jI4zK8JfgHPyQGe+H5/ENbq1A/oiErszhfyki1HAsrkBOrVFLu36Q0LHpQGEE2hd9ReZeDQxJ6BQ0img3ECPxZqLUy0Xm3Y4jcLHK8rJBCd2Qw0/HB1cgox6opV034A9xjGFbB0orkPU5k9t0lHmlfF7IzO+b0AaxIxaLrP8znInvcJJtw+4xYnfMEFVSNdGQ0PYqy2lTmR2zqLVdf8Le6ZfCIyz1wt1yJrYK7ysVqzPfU8dchU8zvE1it0wWD7dMR4hdidu6IDcPaYQZlMPfI/O7aqFSO2ZRa7s2KohYhQ60NqF5vZBTyxBWiC8S+nMOPzVEY0IHiHyEfIM/kXxm49oS5Q/P4R+a0LwcqbtQqR2zqKVd+4gj/ut0oNCB1ohEa4TqYAm2iD9qV/yZ4Y9MaFtCN+OpnLVGi/P7XWHAUsLw2wltEoYorMQG4kSxs6pRgVYTtbTrCHFUfpIOFDpQe6LM2TgEX5X8CKXhR9FfOl8kcNMLeBMwURwfC5OxTfKvKprFgz4rmp5ZDBPNs1YdPZBWUcI34SrbNhLvFFXoYzpXoPNwkeiTzMt7uDqilnZNWxzpZuzUiW4RDjRR9zsQXI9jMU1UB8txoEj2tuJy+aG4K1icrDtUx86DK/Ee5oijZEWizyni6JpWZK200NjSDXpVC7Wya1Oy3mvpQLaR2CL6CRd2g7BiWCsedJa4f0m7oAvERWCxcrM70Soui+cletwgotUcUaWsKzJnlGg0LqiybpWgFnYdhDPwOr5NBxuKvA90i7hQHY2Pu0HwzozBwqlm4Kb6qlJ3XC022li8kw4Wu42fJbqTd9VGrx6NMSKHmllvReqMRhFYWhQ4D51zIKK0myTygv6q907QzoD58vti/ycchMcVKSKKORBRjZV7admL/x5WiAqtE/Jeae1FL0rCP324S2BilwYyAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle \\left( -64.0, \\  -64\\right)$"
      ],
      "text/plain": [
       "(-64.0, -64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your Code Here ###\n",
    "dydx_symbolic_simplified.evalf(subs={X: 1}), -131072+458752-638976+450560-168960+32256-2688+64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JdYOJFyBKgh"
   },
   "source": [
    "## Chain Rule and Autodiff\n",
    "\n",
    "As we've seen in the previous part, symbolic engines computes the derivative using **some** closed form expression of the function. In particular, `sympy` simplified the **recursive** operation and expressed the function $g_n$ in terms of basic operations over the symbolic variable `X`. On the other hand, Autodiff does not necessarily simplify these operations and apply chain rule directly to the latest operation either in forward or backward mode.\n",
    "\n",
    "\n",
    "**Question 5 (4 points):** Using the recursive representation $g_n(x) = f(g_{n-1}(x))$ calculate the derivative of $g_n$ as a function of $g_{n-1}$ and $g^{'}_{n-1}$.  (To be clear, this part is not code; write the mathematical expression below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__gX6dSCyLrt"
   },
   "source": [
    " '### Your Answer Here ###'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEIHD5wqG9fK"
   },
   "source": [
    "**Answer:** Using chain rule we have,\n",
    "$$g_n^{'}(x) = 4 g_{n-1}^{'}(x)(1-g_{n-1}(x)) - 4 g_{n-1}^{'}(x) g_{n-1}(x) = 4 g_{n-1}^{'}(x) - 8g_{n-1}^{'}(x) g_{n-1}(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZc23QPvHjkJ"
   },
   "source": [
    "This implies that we can compute $g_{n}^{'}(x)$ in the forward mode if we could compute $g_{n-1}(x)$ and $g_{n-1}^{'}(x)$ in the forward mode. Indeed, this is possible by applying the same logic to $g_{n-1}^{'}(x)$ and compute it based on $g_{n-2}(x)$ and $g_{n-2}^{'}(x)$; In other words, by augmenting the forward computation graph $g_1 \\to g_2 \\dots \\to g_n$ with $(g_1, g_1^{'})\\to (g_2,g_2^{'}),\\dots \\to (g_n,g_n^{'})$ we can compute the derivative in a forward pass.\n",
    "\n",
    "**Question 6 (8 points):** Modify the functions `op` and `operation` with their counterparts `op_with_grad` and `operation_with_grad` using the logic explained above to compute the derivative of $g_n$. Check your function by evaluating it at $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pP-kBw5SKjxV"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIYAAAAVCAYAAACDi5Z8AAAEfklEQVR4nO3aW4ydUxQH8N+0JUZbU1GNxK2UlqQS6k5aVFsJL+ryIiqlREPUNYS6jDsPvQeJa5UHIiOoSjUaUapScXuQUloTpI1SVUWV1nhY35c58813Zs6cc2Z61PyTk5Wz1tlr772+9a3L3qeusbFRL3qRRZ8i/AXYgP49uJZe9DyORQumZAV5jnEcLsZD+D0jOwBPYx22oRmzsXcZi6qmrlrDaDRhvdjbeizB2SWMnSQeVgsuL2Purtj1I7yC+zCgUJDnGA/gVzyW4Q9LFF2KlZiFtbgWK7BPFxZfTV21htuxDGOwGDOwUDyY0zsZeyDm4bcy5y7Hrg9iP0wrZPbL/Gg4xuFJbM3IHsWQRMG8Av5MXI/7MbXEDVRTVy3hQtyLt3AetmTku3Uwtg7PYCNexk1lzF+OXVfiC1yJh7GD9hHjsmSBL2b4h2KCCEuPZGR3iZQzSWk1STV11RL6CMP+gYu0dwr4u4Px0zBWvO3ZFF4KKrHrCzhIBAW0d4xxwmM+yPDHJnQJ/snItmA59sRJJWygmrpqCafgELyBTTgHt4gwfnInY48UNd0ckYbKQSV2XZ7Q8Smj0DH642is0t5jRyR0dZFFfZXQ4UXk3aWrlnB8Qn/Ax3hdPOzZeB/vYN+ccf3wHL7FbRXMX4ldP0zomJRR6Bj7o6+ooLNoSOjmIpOm/EFF5N2lq5YwJKFTUS+i70CMxJvC6C/ljLsTx2Cy9nVdV1CJXTfjT5FO0NYx0op1UxmLqktoSxlju1NXZ2jW2hqW8nm+A119E1qHC7BUdBefYyK+x2nappUTRJSYIbqG7kRndv0Zg9MvhV1J6q175AxKva0hRwZ7ZX7XEaqpq1KsEW9KqVjXgSx9odbis4xsq4gaU4QzrNCaQlbjji6soRgqtWu9gohV6BgbEprX636Z0GJ5//CEFstv3aWrUpxZRV3pvn4pIk8dpz6hA7TaoJhzPpF85uC6Eucvx659RIr5JmUUOsZ6/Ki1iCnE2wmdkCgprHoH4lThbdluJg/V1FVLWIbt4gHsjr8y8pEJbU7oNjxVRNcoUXe8Jx54KWmmEruOEKnm05RRWGO0iM0NxmGZgWtEGzQUV2dkd4uOZoH23cwwHKHtwU65uuYna5ysNvGTOP9pEAVlIcbjLBHGFye8reLIO+/zWvKbZ5Pv2XOlatqV1hY2da52J59NOD/ZxNcZ2VWi7ZorQvAqnIgzRHianjPhUhws+vvmCnWlTrw9R1YruEHsY7roQlaK/U8U50NXKJ5quoJq2pWIMjvwasrIHnA1iT78kpzBa8QF2/xkshuF584VlfbGEjdVrq6jxEHNoi7M09PYIPYzS9x7pKeZi8TFWl67Wk2UY9cGnCvOXb5LmXU5/8e4VVykjcIn1Vx1BRgkNjUDN+/cpexyuEY4zhi8mzLzbldniVO4e3pmXSVhtLhnmLmzF7KLoV4EgiYFTkH7GoNonSaJvNRfeRc61cZC+ecrvagMQ/G4SD1tkOcYRHdS7mVOL/47WIXGPEGxv/b14n+OfwH63EKkpPUFgwAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$\\displaystyle \\left( 0.0, \\  -64.0\\right)$"
      ],
      "text/plain": [
       "(0.0, -64.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your Code Here ###\n",
    "def op_with_grad(inp, grad):\n",
    "  # inp could be any function of x.\n",
    "  # grad is the corresponsing derivative of inp with respect to x.\n",
    "  # The function should return a tuple where the first element is the `op` applied to inp\n",
    "  # and the second element should be the gradient of op(inp) with respect to x.\n",
    "    return(op(inp), 4*grad-8*grad*inp)\n",
    "    pass\n",
    "\n",
    "    \n",
    "\n",
    "def operation_with_grad(inp, n = 1):\n",
    "  # inp represents x here.\n",
    "  # This function should output a tuple where the first element is the value of g_n(inp)\n",
    "  # and the second element should be the derivative evaluated at inp, i.e. g_n^'(inp)\n",
    "    temp = inp\n",
    "    grad = 1\n",
    "    for i in range(n):\n",
    "        temp, grad = op_with_grad(temp, grad) # apply this operation n times\n",
    "    return (temp, grad)\n",
    "    pass\n",
    "\n",
    "operation_with_grad(1.0, n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcGjDmqTNJHR"
   },
   "source": [
    "Fortunately, `torch` can do last part for us! As was discussed in class, `torch` interprets our code and builds up a computation graph using operations that knows their gradients already and complements each operation with their backward gradient; in order to compute the gradient it follows the following backwards path\n",
    "\n",
    "$$ (g_n, \\frac{\\partial}{\\partial g_n}g_n) \\to (g_{n-1}, \\frac{\\partial}{\\partial g_{n-1}}g_{n}) \\to (g_1, \\frac{\\partial}{\\partial g_1}g_n) \\to (g_0, \\frac{\\partial}{\\partial g_0}g_n) $$\n",
    "\n",
    "where $g_0(x) = x$ is the identity function.\n",
    "\n",
    "**Question 7 (7 points):** Use PyTorch to calculate the derivative of g_n(x) with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "lCqCnqiRR16v"
   },
   "outputs": [],
   "source": [
    "def torch_op(inp):\n",
    "    return 4 * inp * ( 1 - inp )\n",
    "\n",
    "def torch_operation(inp, n = 1):\n",
    "    temp = inp\n",
    "    for i in range(n):\n",
    "        temp = torch_op(temp)\n",
    "        temp.retain_grad()# apply this operation n times\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sd-VpU56lFyB",
    "outputId": "3919daa5-e653-4186-9212-ddabc7fbdd53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-64.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(16.), tensor(4.))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#### Your code here #####\n",
    "# define the function with n=3 iterations as before, and use pytorch to autodiff it!\n",
    "y = torch_op(x)\n",
    "y.retain_grad()\n",
    "z = torch_op(y)\n",
    "z.retain_grad()\n",
    "gn = torch_op(z)\n",
    "# gn = torch_operation(x, n = 3)\n",
    "gn.backward()\n",
    "df_dx = x.grad\n",
    "print(df_dx.numpy())\n",
    "y.grad, z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.8529]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.randn(1,1, requires_grad = True)\n",
    "yy = 3*xx\n",
    "yy.retain_grad()\n",
    "zz = yy**2\n",
    "zz.backward()\n",
    "\n",
    "xx.grad \n",
    "yy.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4076, -1.4076, -0.4076],\n",
      "        [-2.4076, -1.4076, -0.4076]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# create a 2x3 input tensor\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                             [4.0, 5.0, 6.0]])\n",
    "# apply log_softmax along dimension 1\n",
    "output_tensor = torch.nn.functional.log_softmax(input_tensor, dim=1)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, dtype=torch.long).random_(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
