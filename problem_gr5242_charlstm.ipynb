{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5vtG0bbJt9u"
   },
   "source": [
    "# **GR5242 Final Exam: Text Prediction with LSTM networks**\n",
    "\n",
    "**Instructions**: This problem is an individual assignment -- you are to complete this problem on your own, without conferring with your classmates.  You should submit a completed and published notebook to Courseworks; no other files will be accepted.\n",
    "\n",
    "## Description:\n",
    "\n",
    "This homework exercise has 3 primary goals:\n",
    " * Introduce some basic concepts from natural language processing\n",
    " * Get some practice training recurrent neural networks, specifically on text data\n",
    " * Be able to generate fake text data from your favorite author!   \n",
    "\n",
    "By the end of this exercise, you will have a basic, but decent, computer program which can simulate the writing patterns of any author of your choice.\n",
    "\n",
    "Here is an outline of the rest of the exercise.\n",
    " 1. Data loading\n",
    "     - We will start by downloading a text from Project Gutenberg that we will try to model\n",
    "     - Data preprocessing and numerical encoding\n",
    "     - Making training `Dataset` and `DataLoader` objects\n",
    " 3. Learn to generate text with a neural network\n",
    "     - Defining the recurrent network\n",
    "     - Training\n",
    "     - Predicting and sampling text from the model\n",
    "\n",
    "     There are 12 questions (70 points) in total, which include coding and written questions. You can only modify the codes and text within \\### YOUR CODE HERE ### and/or \\### YOUR ANSWER HERE ###.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5589,
     "status": "ok",
     "timestamp": 1700589433143,
     "user": {
      "displayName": "Kamiar Rahnama Rad",
      "userId": "17586984155005782915"
     },
     "user_tz": 300
    },
    "id": "6hwabXFdkMlH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_-QkOsDQrgu"
   },
   "source": [
    "## Character-level language modeling\n",
    "\n",
    "Our goal here is to build a model of language letter-by-letter. Since we may also allow numbers, spaces, and punctuation, it's better to say character-by-character. We will start by fixing an \"alphabet\": the set of allowed characters.\n",
    "\n",
    "In math notation, let's call the alphabet $A$. In code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1700589909307,
     "user": {
      "displayName": "Kamiar Rahnama Rad",
      "userId": "17586984155005782915"
     },
     "user_tz": 300
    },
    "id": "K8RdHuHPRCPJ"
   },
   "outputs": [],
   "source": [
    "alphabet = \" abcdefghijklmnopqrstuvwxyz1234567890.,!?:;ABCDEFGHIJKLMNOPQRSTUVWXYZ\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sitlEu4BJsBm"
   },
   "source": [
    "# Section 1: Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oryq1WYLNojq"
   },
   "source": [
    "We will start by loading our data. Data will be provided with the assignment.\n",
    "\n",
    "If you are having issues with those data or would like another dataset to work with, you can also try downloading training data from Project Gutenberg: https://www.gutenberg.org/. Project Gutenberg is a free repository of public domain books. Find any book you like, and download it in Plain Text UTF-8 format.\n",
    "\n",
    "For example, we can use Shakespeare's complete works: https://www.gutenberg.org/ebooks/100. There is a link on that page to the Plain Text format data. \n",
    "\n",
    "Download the .txt file, and then upload it from your computer to colab (click at left on the File icon, then click the upload icon).  \n",
    "\n",
    "*Important*: whichever work you choose, make sure you have enough data! The size of your plain text file should be at least 2MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1700589912390,
     "user": {
      "displayName": "Kamiar Rahnama Rad",
      "userId": "17586984155005782915"
     },
     "user_tz": 300
    },
    "id": "NeGF0qSbJqqB"
   },
   "outputs": [],
   "source": [
    "# after uploading your file to Colab, set this variable to its path:\n",
    "txt_path = \"C:/Users/zha/Downloads/test_text.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNYDQKRSXBk"
   },
   "source": [
    "Let's load the text and see what it says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 577,
     "status": "error",
     "timestamp": 1700589914946,
     "user": {
      "displayName": "Kamiar Rahnama Rad",
      "userId": "17586984155005782915"
     },
     "user_tz": 300
    },
    "id": "7RKK_A1NShRW",
    "outputId": "43b53e24-c731-4ec7-b0c8-20bafaee7252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is 2648423 characters long.\n",
      "\n",
      "A sample from the middle:\n",
      "\n",
      "there was another reason why the theory of verbal inflation didnâ€™ t seem\n",
      "to hold water: it was only \n"
     ]
    }
   ],
   "source": [
    "with open(txt_path, encoding=\"utf-8\") as txt_file:\n",
    "    text = txt_file.read()\n",
    "\n",
    "print(\"text is\", len(text), \"characters long.\")\n",
    "print()\n",
    "print(\"A sample from the middle:\")\n",
    "print()\n",
    "print(text[len(text) // 2 : len(text) // 2 + 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7viKRe94P_Cx"
   },
   "source": [
    "### Data standardization\n",
    "\n",
    "Now, we will clean the data: removing extra spaces and linebreaks, and getting rid of characters which are not in our alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5m3ZqU7RS2xA"
   },
   "outputs": [],
   "source": [
    "# remove extra characters by replacing them with spaces\n",
    "text = re.sub(rf\"[^{alphabet}]\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tgMpHS2O67G"
   },
   "source": [
    "Let's see how it looks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TiJEL-O-O6wL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there was another reason why the theory of verbal inflation didn  t seem\n",
      "to hold water: it was only \n"
     ]
    }
   ],
   "source": [
    "print(text[len(text) // 2 : len(text) // 2 + 100])\n",
    "x_prompt = text[len(text) // 2 : len(text) // 2 + 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JFrb5Jr92F8"
   },
   "source": [
    "### Numerical encoding\n",
    "\n",
    "Unfortunately, neural networks don't understand text. So, we need to convert our characters to numerical values. Here are some helper functions for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1LjvsaNy91to"
   },
   "outputs": [],
   "source": [
    "# let's build a dictionary mapping characters to integers\n",
    "char2int = {c: i for i, c in enumerate(alphabet)}\n",
    "alphabet_array = np.array([c for c in alphabet])\n",
    "\n",
    "# this function will turn a string into a numpy array of integers\n",
    "def int_encode(string):\n",
    "    if any(c not in char2int for c in string):\n",
    "        raise ValueError(\n",
    "            \"Found a character which was not in the alphabet in the input \"\n",
    "            f\"to int_encode. Valid alphabet characters: {alphabet}\"\n",
    "          )\n",
    "    return np.array([char2int[c] for c in string])\n",
    "\n",
    "# this function will decode a numpy array of integers back to a string\n",
    "def int_decode(int_array):\n",
    "    return ''.join(alphabet_array[int_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsSPJalSWFUm"
   },
   "source": [
    "(Question 1a: 4 points) Test out `int_encode` by passing `test_string` in and printing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vU8Umqsw-4CN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  8,  5, 18,  5,  0, 23,  1, 19,  0,  1, 14, 15, 20,  8,  5, 18,\n",
       "        0, 18,  5,  1, 19, 15, 14,  0, 23,  8, 25,  0, 20,  8,  5,  0, 20,\n",
       "        8,  5, 15, 18, 25,  0, 15,  6,  0, 22,  5, 18,  2,  1, 12,  0,  9,\n",
       "       14,  6, 12,  1, 20,  9, 15, 14,  0,  4,  9,  4, 14,  0,  0, 20,  0,\n",
       "       19,  5,  5, 13, 69, 20, 15,  0,  8, 15, 12,  4,  0, 23,  1, 20,  5,\n",
       "       18, 41,  0,  9, 20,  0, 23,  1, 19,  0, 15, 14, 12, 25,  0, 14,  5,\n",
       "        3,  5, 19, 19,  1, 18, 25,  0, 20, 15,  0, 19,  1, 12, 20,  0, 20,\n",
       "        8,  5,  0,  9, 14,  4,  9, 22,  9,  4, 21,  1, 12,  0, 21, 14,  9,\n",
       "       20, 19,  0, 15,  6,  0, 20,  8,  5,  0, 23, 15, 18,  4, 69,  7, 18,\n",
       "       15, 21, 16,  0,  0, 18,  5,  4,  5, 13, 16, 20,  9, 15, 14,  0,  0,\n",
       "       23,  9, 20,  8,  0,  1,  0, 19, 13,  1, 12, 12, 38,  0,  9, 14, 14,\n",
       "       15,  3,  5, 14, 20,  0, 12,  1,  3, 11,  0, 15,  6,  0,  7, 18,  1,\n",
       "       22,  9, 20, 25,  0, 38,  0,  1, 14,  4,  0, 20,  8,  5, 25, 69,  9,\n",
       "       14, 19, 20,  1, 14, 20, 12, 25,  0,  3,  1, 13,  5,  0, 20, 18,  9,\n",
       "       16, 16,  9, 14,  7, 12, 25,  0,  6, 18, 15, 13,  0, 20,  8,  5,  0,\n",
       "       20, 15, 14,  7, 21,  5, 37,  0,  0, 67,  0, 15, 21,  0, 22,  5,  0,\n",
       "       10, 21, 19, 20,  0, 19,  1, 22,  5,  4,  0, 13, 25,  0, 19, 15, 21,\n",
       "       12, 39,  0,  0, 15, 18, 69, 19, 15, 13,  5,  0, 19, 21,  3,  8, 42,\n",
       "        0, 23,  8, 15,  0,  8,  1, 19,  0, 14, 15, 20,  0, 19,  1,  9,  4,\n",
       "        0, 19, 15, 13,  5, 20,  8,  9, 14,  7,  0, 15,  6,  0, 20,  8,  5,\n",
       "        0, 19, 15, 18, 20,  0,  1, 20,  0, 15, 14,  5,  0, 20,  9, 13,  5,\n",
       "        0, 15, 18,  0,  1, 14, 15, 20,  8,  5, 18,  0, 38, 69, 16, 18, 15,\n",
       "       22,  9,  4,  5,  4,  0, 15,  6,  0,  3, 15, 21, 18, 19,  5,  0, 20,\n",
       "        8,  1, 20,  0,  9, 20,  0, 18,  5,  6,  5, 18, 19,  0, 20, 15,  0,\n",
       "       14, 15, 20,  8,  9, 14,  7,  0, 13, 15, 18,  5,  0, 20,  8,  1, 14,\n",
       "        0, 20,  8,  5,  0, 18,  5, 12,  9,  5,  6,  0,  1,  6, 20,  5, 18,\n",
       "        0,  1,  0, 20,  5, 14,  0, 69, 13,  9, 14, 21, 20,  5,  0, 23,  1,\n",
       "        9, 20,  0, 15, 18,  0, 19, 15, 13,  5,  0,  5, 17, 21,  1, 12, 12,\n",
       "       25,  0, 19, 12,  9,  7,  8, 20,  0,  9, 14,  3, 15, 14, 22,  5, 14,\n",
       "        9,  5, 14,  3,  5,  0, 20,  8,  1, 20,  0,  8,  1, 19,  0,  2,  5,\n",
       "        5, 14,  0,  2, 18, 15, 21,  7,  8, 20,  0, 20, 15, 69,  1, 14,  0,\n",
       "        5, 14,  4, 37,  0, 56, 15, 23,  0, 20,  8,  5,  0, 49,  5, 14,  5,\n",
       "       18,  1, 12,  0, 18,  5,  1, 12,  9, 26,  5,  4,  0, 20,  8,  1, 20,\n",
       "        0,  9, 20,  0, 23,  1, 19,  0, 14, 15, 20,  0, 19, 15,  0, 13, 21,\n",
       "        3,  8,  0, 20,  8,  5,  0, 23, 15, 18,  4, 19,  0, 20,  8,  1, 20,\n",
       "       69, 15,  6,  6,  5, 14,  4,  5,  4,  0,  1,  0,  8,  5,  1, 12, 20,\n",
       "        8, 25,  0,  3, 15, 13, 13, 15, 14,  0, 19,  5, 14, 19,  5,  0,  1,\n",
       "       19,  0, 20,  8,  5,  9, 18,  0,  1,  2, 19, 21, 18,  4,  0,  3, 12,\n",
       "        1,  9, 13,  0, 20, 15,  0,  2,  5,  9, 14,  7,  0, 20,  1, 11,  5,\n",
       "       14, 69, 12,  9, 20,  5, 18,  1, 12, 12, 25,  0, 37,  0, 65,  8,  5,\n",
       "       14,  0, 61, 20, 21, 13, 13,  0,  1, 19, 11,  5,  4,  0,  8,  9, 13,\n",
       "       19,  5, 12,  6,  0, 23,  8,  5, 18,  5,  0,  8,  5,  0,  8,  1,  4,\n",
       "        0,  5, 22,  5, 18,  0,  3, 15, 13,  5,  0,  1,  3, 18, 15, 19, 19,\n",
       "        0, 19, 21,  3,  8, 69, 20,  1, 12, 11,  0, 15,  6,  0, 18,  5,  4,\n",
       "        5, 13, 16, 20,  9, 15, 14,  0, 15, 18,  0, 19,  1, 12, 22,  1, 20,\n",
       "        9, 15, 14, 38,  0, 15, 20,  8,  5, 18,  0, 20,  8,  1, 14,  0,  1,\n",
       "       20,  0, 46,  9, 15, 20,  9, 13,  1,  0,  0, 19,  0, 15, 18,  0,  9,\n",
       "       14,  0, 16, 15, 12,  9, 20,  9,  3, 19, 38,  0,  8,  5, 69, 18,  5,\n",
       "        1, 12,  9, 26,  5,  4,  0, 20,  8,  1, 20,  0,  9, 20,  0,  8,  1,\n",
       "        4,  0,  2,  5,  5, 14,  0,  9, 14,  0,  3,  8, 21, 18,  3,  8,  5,\n",
       "       19,  0, 15, 18,  0,  3,  1,  6,  0, 19, 38,  0,  9, 14,  0,  1, 18,\n",
       "       20,  0, 10, 15, 21, 18, 14,  1, 12, 19, 38,  0,  1, 14,  4,  0,  9,\n",
       "       14,  0, 20,  8,  5, 69,  2, 15, 15, 11, 19,  0, 15,  6,  0, 46, 18,\n",
       "        0, 37,  0, 43, 18, 14,  8,  5,  9, 13, 38,  0, 23,  8,  9,  3,  8,\n",
       "        0,  8,  5,  0,  8,  1,  4,  0, 18,  5,  1,  4,  0, 23,  9, 20,  8,\n",
       "        0,  1,  4, 13,  9, 18,  1, 20,  9, 15, 14, 37,  0, 50,  5,  0, 14,\n",
       "       15, 23,  0, 18,  5,  1, 12,  9, 26,  5,  4, 69, 20,  8,  1, 20,  0,\n",
       "       19, 21,  3,  8,  0, 23, 15, 18,  4, 19,  0, 18,  5,  6,  5, 18,  0,\n",
       "       14, 15, 20,  0, 20, 15,  0,  1,  0, 19,  9, 13, 16, 12])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test these out!\n",
    "### YOUR CODE HERE ###\n",
    "test_int = int_encode(x_prompt)\n",
    "test_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGFYXZPcWL8Z"
   },
   "source": [
    "(Question 1b: 4 points) Decode the result from the last cell using `int_decode` to make sure it is the same as `test_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "392dWPKn0J85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "int_decode(test_int)== x_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWxJykne_VKY"
   },
   "source": [
    "Is the decoding the same as `test_string`? It should -- you have a bug above if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIqRJLtyAC--"
   },
   "source": [
    "### Make a training dataset\n",
    "\n",
    "First, we make a numerical encoded version of the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "JOr_T3ddALvQ"
   },
   "outputs": [],
   "source": [
    "enctext = int_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1FLc9MABXKu"
   },
   "source": [
    "Use `torch.tensor` to make it into a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ORDJL4_0BWTK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([60, 57, 44,  ..., 46, 63, 69], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "enctext = torch.tensor(enctext)\n",
    "print(enctext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk3z0bACJyQc",
    "tags": []
   },
   "source": [
    "# Section 2: Training a NN\n",
    "\n",
    "Our model will work as follows:\n",
    " - One-hot encoded input gets passed into a linear embedding layer. These two operations are combined with the `Embedding` layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    " - LSTM cell\n",
    " - Linear decoder layer\n",
    "\n",
    "PyTorch has two main ways of interfacing with recurrent networks. In the case of LSTMs, those are:\n",
    " - the LSTM layer https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    " - the LSTMCell layer https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "\n",
    "Both models are sequential: the goal is to process a batch of sequences of input features and produce a batch of sequences of output features. The `LSTM` class makes this simple and easy, and the `LSTMCell` class gives more control by allowing you to process the sequences one element at a time. We will use the `LSTM` layer to keep things simple, but keep in mind that some of what we do could be made more efficient with `LSTMCell`.\n",
    "\n",
    "The inputs and outputs to recurrent networks in `pytorch` have shape: `(batch_dimension, sequence_dimension, feature_dimension)`. In this case, our feature dimension is `len(alphabet)`.\n",
    "\n",
    "Something to keep in mind: the output of this network will be stateful! In each batch, the `k`th output along the sequence dimension will be the logits for predicting the `k+1`th input in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "F80hsi-FH4WA"
   },
   "outputs": [],
   "source": [
    "# We will use this constant below\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "_ZUt-1--LP32"
   },
   "outputs": [],
   "source": [
    "# Defining some parameters about data batching, explained in the next section\n",
    "# Note: after you get the entire assignment working, you can make these\n",
    "# bigger and train for longer, to get better performance\n",
    "SEQUENCE_LENGTH = 32\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2ORoPzNJkKP"
   },
   "source": [
    "### Making the dataset of (input, target) pairs\n",
    "\n",
    "To train the model, we need to make a [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) containing input and target sequences. Our input sequences will be sequences of length `SEQUENCE_LENGTH` containing int-encoded characters from the input. Our target sequences will be the \"next characters\" corresponding to the input sequence: so, if the input sequence is the 10th, 11th, ... characters, then the target sequence is the 11th, 12th, ... characters.\n",
    "\n",
    "We will walk through creating these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PlSDJOjL_Fb"
   },
   "source": [
    "(Question 2a: 8 points) Write a `batch` function for a `torch` tensor, which we defined above, to make disjoint consecutive sequences of consecutive characters of length `SEQUENCE_LENGTH`. These disjoint sequences should become the rows of a two-dimensional tensor.\n",
    "\n",
    "[`torch.split()`](https://pytorch.org/docs/stable/generated/torch.split.html) and [`torch.vstack()`](https://pytorch.org/docs/stable/generated/torch.vstack.html) may be useful. You may also write this function recursively.\n",
    "\n",
    "Remember to be careful of the edge case that arises when `len(enctext) % SEQUENCE_LENGTH != 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "dX6fJc84W7Lo"
   },
   "outputs": [],
   "source": [
    "def batch(enctext, SEQUENCE_LENGTH):\n",
    "    # YOUR CODE HERE\n",
    "    if len(enctext) % SEQUENCE_LENGTH != 0:\n",
    "        return torch.vstack(torch.split(enctext, SEQUENCE_LENGTH)[:-1])\n",
    "    else:\n",
    "        return torch.vstack(torch.split(enctext, SEQUENCE_LENGTH))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n24Q5ySjU2ed"
   },
   "source": [
    "(Question 2b: 8 points) Now, use batch to create input and target sequences which have been offset by 1 element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "BkjMZfS3XAae"
   },
   "outputs": [],
   "source": [
    "input_seqs  = batch(enctext, SEQUENCE_LENGTH) # YOUR CODE HERE\n",
    "target_seqs = batch(enctext[1:], SEQUENCE_LENGTH) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geD7pQ7TVH1Z"
   },
   "source": [
    "(Question 2c: 6 points) Now, use the `torch` builtin class [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) to create a dataset called `pairs` of (input, target) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "B5rrw7OLXGjS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x215485fab80>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = TensorDataset(input_seqs, target_seqs) # YOUR CODE HERE\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX1_4b0oVtdv"
   },
   "source": [
    "(Question 2d: 4 points) Finally, define a `torch.utils.data.DataLoader` object to generate batches of pairs of length `BATCH_SIZE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "pE547bYpXJge"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x215485aabe0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(pairs, BATCH_SIZE, shuffle=True) # YOUR CODE HERE\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may uncomment the below cell if you would like to understand the structure of the `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5, 19, 38,  0, 20,  8,  1, 20,  0,  3, 12,  1,  9, 13,  0, 13, 21, 19,\n",
      "         20,  0,  5, 14, 10, 15, 25,  0, 20,  8,  5,  0, 16, 18],\n",
      "        [ 6,  0, 20,  8,  1, 20,  0, 20,  1, 11,  5, 19,  0, 15, 14,  0, 20,  8,\n",
      "          5,  0, 19,  8,  1, 16,  5,  0, 15,  6, 69,  5, 22,  5],\n",
      "        [ 8,  5,  0,  9, 14,  1,  2,  9, 12,  9, 20, 25,  0, 15,  6,  0,  8, 21,\n",
      "         13,  1, 14,  0,  2,  5,  9, 14,  7, 19,  0, 20, 15,  0],\n",
      "        [19, 16, 15, 14, 19,  9,  2,  9, 12,  9, 20, 25,  0, 37,  0, 43, 14,  4,\n",
      "          0, 19, 15,  0, 20,  8,  5, 25,  0,  6,  9,  5, 18,  3],\n",
      "        [ 5, 25,  5,  0,  2,  1, 18,  5, 12, 25,  0, 16,  5, 18,  3,  5,  9, 22,\n",
      "          5,  4,  0,  2,  5,  3,  1, 21, 19,  5,  0,  9, 20,  0],\n",
      "        [13, 19, 20,  1, 14,  3,  5, 19,  0,  1, 14, 25, 15, 14,  5,  0,  9, 19,\n",
      "         69,  3,  1, 16,  1,  2, 12,  5,  0, 15,  6,  0,  1, 14],\n",
      "        [12, 25,  0, 18,  5, 13,  1,  9, 14, 19,  0,  9, 19,  0,  6, 15, 18, 13,\n",
      "         21, 12,  1, 19, 37, 69, 65,  8,  1, 20,  0, 20,  8,  5],\n",
      "        [15, 15, 19,  2, 18, 21,  7,  7,  5, 18, 69,  8,  5,  1, 18,  4,  0, 22,\n",
      "         15,  9,  3,  5, 19,  0, 15, 18,  0, 13, 21, 19,  9,  3],\n",
      "        [12, 15,  1, 20,  8,  9, 14,  7, 38,  0,  9, 14, 22, 15, 12, 21, 14, 20,\n",
      "          1, 18,  9, 12, 25,  0, 19,  8, 15,  3, 11,  5,  4,  0],\n",
      "        [ 4,  0, 20, 15,  0, 20,  8,  5,  0, 16,  1, 19, 19,  9, 14,  7,  0,  7,\n",
      "          5, 14, 20, 12,  5, 13,  5, 14,  0,  8,  5, 18,  0, 21],\n",
      "        [20, 15, 15, 11,  0, 20, 15,  0, 16, 21, 18, 19, 21,  9, 14,  7,  0, 60,\n",
      "          1,  3,  8,  5, 12,  0, 23,  9, 20,  8,  0,  6,  1, 14],\n",
      "        [ 3, 15, 14,  3, 12, 21,  4,  5,  4,  0,  6,  9, 18, 13, 12, 25,  0, 38,\n",
      "          0, 15, 14,  0,  1,  0,  8,  1, 16, 16, 25,  0,  9, 14],\n",
      "        [15, 18, 19,  0, 15,  6,  0, 20,  8,  5,  0, 58, 15, 16, 21, 12,  1, 20,\n",
      "          9, 15, 14,  0,  9, 14,  0, 45, 15, 14, 14,  5,  3, 20],\n",
      "        [ 2,  9, 22,  1, 12,  5, 14,  3,  5,  0, 23,  1, 19,  0, 19, 20,  9, 12,\n",
      "         12,  0,  8,  5, 18,  0,  6,  1,  3,  5, 38,  0, 19, 15],\n",
      "        [ 6, 14,  5, 19, 19, 37,  0, 69, 44, 21, 20,  0, 23,  8,  5, 14,  0,  8,\n",
      "          5,  0,  5, 14, 20,  5, 18,  5,  4,  0, 45, 15, 21, 14],\n",
      "        [13,  5, 14,  0,  8,  1, 22,  5,  0, 20, 15,  0, 19,  8,  1, 18,  5,  0,\n",
      "          1,  0, 18, 15, 15, 13,  0,  6, 15, 18,  0,  1, 14, 25]],\n",
      "       dtype=torch.int32) tensor([[19, 38,  0, 20,  8,  1, 20,  0,  3, 12,  1,  9, 13,  0, 13, 21, 19, 20,\n",
      "          0,  5, 14, 10, 15, 25,  0, 20,  8,  5,  0, 16, 18,  9],\n",
      "        [ 0, 20,  8,  1, 20,  0, 20,  1, 11,  5, 19,  0, 15, 14,  0, 20,  8,  5,\n",
      "          0, 19,  8,  1, 16,  5,  0, 15,  6, 69,  5, 22,  5, 18],\n",
      "        [ 5,  0,  9, 14,  1,  2,  9, 12,  9, 20, 25,  0, 15,  6,  0,  8, 21, 13,\n",
      "          1, 14,  0,  2,  5,  9, 14,  7, 19,  0, 20, 15,  0, 12],\n",
      "        [16, 15, 14, 19,  9,  2,  9, 12,  9, 20, 25,  0, 37,  0, 43, 14,  4,  0,\n",
      "         19, 15,  0, 20,  8,  5, 25,  0,  6,  9,  5, 18,  3,  5],\n",
      "        [25,  5,  0,  2,  1, 18,  5, 12, 25,  0, 16,  5, 18,  3,  5,  9, 22,  5,\n",
      "          4,  0,  2,  5,  3,  1, 21, 19,  5,  0,  9, 20,  0,  7],\n",
      "        [19, 20,  1, 14,  3,  5, 19,  0,  1, 14, 25, 15, 14,  5,  0,  9, 19, 69,\n",
      "          3,  1, 16,  1,  2, 12,  5,  0, 15,  6,  0,  1, 14, 25],\n",
      "        [25,  0, 18,  5, 13,  1,  9, 14, 19,  0,  9, 19,  0,  6, 15, 18, 13, 21,\n",
      "         12,  1, 19, 37, 69, 65,  8,  1, 20,  0, 20,  8,  5, 25],\n",
      "        [15, 19,  2, 18, 21,  7,  7,  5, 18, 69,  8,  5,  1, 18,  4,  0, 22, 15,\n",
      "          9,  3,  5, 19,  0, 15, 18,  0, 13, 21, 19,  9,  3,  0],\n",
      "        [15,  1, 20,  8,  9, 14,  7, 38,  0,  9, 14, 22, 15, 12, 21, 14, 20,  1,\n",
      "         18,  9, 12, 25,  0, 19,  8, 15,  3, 11,  5,  4,  0,  9],\n",
      "        [ 0, 20, 15,  0, 20,  8,  5,  0, 16,  1, 19, 19,  9, 14,  7,  0,  7,  5,\n",
      "         14, 20, 12,  5, 13,  5, 14,  0,  8,  5, 18,  0, 21,  7],\n",
      "        [15, 15, 11,  0, 20, 15,  0, 16, 21, 18, 19, 21,  9, 14,  7,  0, 60,  1,\n",
      "          3,  8,  5, 12,  0, 23,  9, 20,  8,  0,  6,  1, 14, 20],\n",
      "        [15, 14,  3, 12, 21,  4,  5,  4,  0,  6,  9, 18, 13, 12, 25,  0, 38,  0,\n",
      "         15, 14,  0,  1,  0,  8,  1, 16, 16, 25,  0,  9, 14, 19],\n",
      "        [18, 19,  0, 15,  6,  0, 20,  8,  5,  0, 58, 15, 16, 21, 12,  1, 20,  9,\n",
      "         15, 14,  0,  9, 14,  0, 45, 15, 14, 14,  5,  3, 20,  9],\n",
      "        [ 9, 22,  1, 12,  5, 14,  3,  5,  0, 23,  1, 19,  0, 19, 20,  9, 12, 12,\n",
      "          0,  8,  5, 18,  0,  6,  1,  3,  5, 38,  0, 19, 15,  0],\n",
      "        [14,  5, 19, 19, 37,  0, 69, 44, 21, 20,  0, 23,  8,  5, 14,  0,  8,  5,\n",
      "          0,  5, 14, 20,  5, 18,  5,  4,  0, 45, 15, 21, 14, 20],\n",
      "        [ 5, 14,  0,  8,  1, 22,  5,  0, 20, 15,  0, 19,  8,  1, 18,  5,  0,  1,\n",
      "          0, 18, 15, 15, 13,  0,  6, 15, 18,  0,  1, 14, 25,  0]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[ 0, 18,  5,  1, 12, 12, 25,  0, 21, 16,  0, 20, 15, 38,  0,  0, 19,  8,\n",
      "          5,  0, 19,  1,  9,  4,  0,  9, 14,  0, 20,  8,  5, 69],\n",
      "        [ 5,  4, 15, 13, 39,  0,  0, 50,  5,  0,  8,  1,  4,  0, 19,  1,  9,  4,\n",
      "          0, 20,  8,  9, 19,  0, 23,  9, 20,  8,  0,  1,  0, 19],\n",
      "        [ 6,  0, 20,  8,  5,  0, 19,  5, 14, 19,  5,  0, 15,  6,  0, 18,  5,  1,\n",
      "         12,  9, 20, 25,  0, 37,  0, 44, 21, 20,  0,  9,  6,  0],\n",
      "        [ 7,  0,  6, 15, 18,  5,  8,  5,  1,  4, 19, 38,  0, 20,  8,  5,  0, 20,\n",
      "         23,  9, 20,  3,  8,  9, 14,  7,  0,  2, 15,  4,  9,  5],\n",
      "        [19,  8, 15, 21, 12,  4,  5, 18,  0,  2,  5,  6, 15, 18,  5,  0, 19,  8,\n",
      "          5,  0, 20, 15, 21,  3,  8,  5,  4,  0, 20,  8,  5,  0],\n",
      "        [ 2, 12,  1,  3, 11,  1, 13, 15, 15, 18,  0, 23,  1, 19,  0, 18,  5,  1,\n",
      "         12, 37,  0, 61, 15, 13,  5,  0, 25,  5,  1, 18, 19,  0],\n",
      "        [ 0, 14,  5, 23,  0, 16,  5, 15, 16, 12,  5, 38,  0,  8,  5,  0,  4,  9,\n",
      "          4,  0,  9, 20,  0, 14, 15, 20,  0, 19,  9, 13, 16, 12],\n",
      "        [20,  0, 15, 18,  0, 23,  8,  5, 14,  0, 15, 14,  5,  0, 23,  9, 12, 12,\n",
      "          0,  3, 15, 13,  5,  0,  9, 14, 20, 15,  0,  1, 69,  3],\n",
      "        [ 4,  0,  3, 18, 15, 19, 19, 12, 25,  0, 38,  0,  1, 19,  0, 20,  8, 15,\n",
      "         21,  7,  8,  0, 20,  8,  9, 19,  0, 23,  5, 18,  5,  0],\n",
      "        [ 1, 21, 20,  9,  6, 21, 12,  0, 19,  8, 15, 21, 12,  4,  5, 18, 19, 37,\n",
      "          0,  0, 56, 15, 20,  8,  9, 14,  7,  0, 19, 15,  0, 22],\n",
      "        [12, 12,  0, 17, 21,  1, 12,  9, 20,  9,  5, 19,  0, 15,  6,  0, 20,  5,\n",
      "         13, 16,  5, 18,  1, 13,  5, 14, 20, 69,  1, 14,  4,  0],\n",
      "        [18,  4,  5,  4,  0, 20,  8,  5,  0,  3, 18, 21,  4,  5,  0, 16,  8, 25,\n",
      "         19,  9,  3,  1, 12,  0, 19,  5, 14, 19,  1, 20,  9, 15],\n",
      "        [15,  0,  9, 14,  0,  8,  9, 19,  0,  3, 15, 14, 22,  5, 18, 19,  1, 20,\n",
      "          9, 15, 14, 19,  0, 23,  9, 20,  8,  0, 48,  9, 19,  3],\n",
      "        [ 1, 21, 19,  5,  0,  9, 20,  0,  9, 19,  0,  1, 14,  0,  9, 14,  3, 18,\n",
      "          5,  4,  9,  2, 12, 25,  0, 22,  9,  3,  9, 15, 21, 19],\n",
      "        [ 0,  3, 15, 21, 18, 19,  5,  0,  8,  5,  0, 23,  9, 12, 12,  0, 14,  5,\n",
      "         22,  5, 18,  0,  4,  5, 14, 25, 69, 20,  8,  1, 20,  0],\n",
      "        [ 8,  0,  9, 20,  0, 23,  5, 18,  5, 69,  8,  5, 18,  0, 15, 23, 14, 38,\n",
      "          0,  9, 20,  0,  7,  1, 22,  5,  0,  8,  5, 18,  0, 13]],\n",
      "       dtype=torch.int32) tensor([[18,  5,  1, 12, 12, 25,  0, 21, 16,  0, 20, 15, 38,  0,  0, 19,  8,  5,\n",
      "          0, 19,  1,  9,  4,  0,  9, 14,  0, 20,  8,  5, 69,  4],\n",
      "        [ 4, 15, 13, 39,  0,  0, 50,  5,  0,  8,  1,  4,  0, 19,  1,  9,  4,  0,\n",
      "         20,  8,  9, 19,  0, 23,  9, 20,  8,  0,  1,  0, 19, 13],\n",
      "        [ 0, 20,  8,  5,  0, 19,  5, 14, 19,  5,  0, 15,  6,  0, 18,  5,  1, 12,\n",
      "          9, 20, 25,  0, 37,  0, 44, 21, 20,  0,  9,  6,  0, 20],\n",
      "        [ 0,  6, 15, 18,  5,  8,  5,  1,  4, 19, 38,  0, 20,  8,  5,  0, 20, 23,\n",
      "          9, 20,  3,  8,  9, 14,  7,  0,  2, 15,  4,  9,  5, 19],\n",
      "        [ 8, 15, 21, 12,  4,  5, 18,  0,  2,  5,  6, 15, 18,  5,  0, 19,  8,  5,\n",
      "          0, 20, 15, 21,  3,  8,  5,  4,  0, 20,  8,  5,  0, 20],\n",
      "        [12,  1,  3, 11,  1, 13, 15, 15, 18,  0, 23,  1, 19,  0, 18,  5,  1, 12,\n",
      "         37,  0, 61, 15, 13,  5,  0, 25,  5,  1, 18, 19,  0,  1],\n",
      "        [14,  5, 23,  0, 16,  5, 15, 16, 12,  5, 38,  0,  8,  5,  0,  4,  9,  4,\n",
      "          0,  9, 20,  0, 14, 15, 20,  0, 19,  9, 13, 16, 12, 25],\n",
      "        [ 0, 15, 18,  0, 23,  8,  5, 14,  0, 15, 14,  5,  0, 23,  9, 12, 12,  0,\n",
      "          3, 15, 13,  5,  0,  9, 14, 20, 15,  0,  1, 69,  3,  5],\n",
      "        [ 0,  3, 18, 15, 19, 19, 12, 25,  0, 38,  0,  1, 19,  0, 20,  8, 15, 21,\n",
      "          7,  8,  0, 20,  8,  9, 19,  0, 23,  5, 18,  5,  0, 10],\n",
      "        [21, 20,  9,  6, 21, 12,  0, 19,  8, 15, 21, 12,  4,  5, 18, 19, 37,  0,\n",
      "          0, 56, 15, 20,  8,  9, 14,  7,  0, 19, 15,  0, 22,  5],\n",
      "        [12,  0, 17, 21,  1, 12,  9, 20,  9,  5, 19,  0, 15,  6,  0, 20,  5, 13,\n",
      "         16,  5, 18,  1, 13,  5, 14, 20, 69,  1, 14,  4,  0, 12],\n",
      "        [ 4,  5,  4,  0, 20,  8,  5,  0,  3, 18, 21,  4,  5,  0, 16,  8, 25, 19,\n",
      "          9,  3,  1, 12,  0, 19,  5, 14, 19,  1, 20,  9, 15, 14],\n",
      "        [ 0,  9, 14,  0,  8,  9, 19,  0,  3, 15, 14, 22,  5, 18, 19,  1, 20,  9,\n",
      "         15, 14, 19,  0, 23,  9, 20,  8,  0, 48,  9, 19,  3,  8],\n",
      "        [21, 19,  5,  0,  9, 20,  0,  9, 19,  0,  1, 14,  0,  9, 14,  3, 18,  5,\n",
      "          4,  9,  2, 12, 25,  0, 22,  9,  3,  9, 15, 21, 19,  0],\n",
      "        [ 3, 15, 21, 18, 19,  5,  0,  8,  5,  0, 23,  9, 12, 12,  0, 14,  5, 22,\n",
      "          5, 18,  0,  4,  5, 14, 25, 69, 20,  8,  1, 20,  0, 20],\n",
      "        [ 0,  9, 20,  0, 23,  5, 18,  5, 69,  8,  5, 18,  0, 15, 23, 14, 38,  0,\n",
      "          9, 20,  0,  7,  1, 22,  5,  0,  8,  5, 18,  0, 13, 21]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[51,  0,  4,  5,  6,  5, 14,  4,  0, 25, 15, 21, 38,  0,  6, 18,  5,  1,\n",
      "         11,  9, 19,  8, 14,  5, 19, 19, 69,  1, 14,  4,  0,  1],\n",
      "        [14,  0,  8,  9, 19,  0, 16,  1, 18, 20, 38,  0,  1, 14,  4,  0,  8,  5,\n",
      "          0, 18,  5,  1,  3, 20,  5,  4,  0, 20, 15,  0, 20,  8],\n",
      "        [ 5,  0,  6,  9, 18, 13,  0, 20, 15,  0, 13,  1, 18, 18, 25,  0,  1,  0,\n",
      "         16, 18, 15, 13,  9, 14,  5, 14, 20,  0, 43, 13,  5, 18],\n",
      "        [ 0, 20,  8,  5,  0,  4,  1, 13, 13,  5,  4,  0, 21, 16,  0, 19,  9, 12,\n",
      "          5, 14,  3,  5,  0, 15,  6,  0, 20,  8,  5,  0, 16, 18],\n",
      "        [23,  1, 19,  0, 14, 15, 20,  8,  9, 14,  7,  0, 23, 18, 15, 14,  7,  0,\n",
      "         23,  9, 20,  8,  0, 20,  8,  1, 20, 69,  5,  9, 20,  8],\n",
      "        [ 9, 19, 15, 14,  5, 18,  0, 37,  0, 61,  9,  5,  7, 13, 21, 14,  4,  0,\n",
      "          7, 15, 20,  0, 13,  5, 69,  1,  0, 12,  5, 20, 20,  5],\n",
      "        [ 8,  5, 13,  0, 21, 16, 38,  0,  1, 19,  0,  9, 19,  0,  4, 15, 14,  5,\n",
      "          0,  9, 14,  0, 20,  8,  5,  0, 13, 15, 18,  1, 12, 69],\n",
      "        [14,  7,  0,  1,  4, 22,  1, 14, 20,  1,  7,  5,  0,  6, 15, 18,  0,  8,\n",
      "          9, 19,  0,  3, 15, 13, 13,  9, 19, 19,  9, 15, 14,  0],\n",
      "        [12,  9, 11,  5,  0,  1, 12, 12,  0,  2,  1, 14, 11,  0,  4,  9, 18,  5,\n",
      "          3, 20, 15, 18, 19,  0,  2,  5,  6, 15, 18,  5,  0, 20],\n",
      "        [ 0,  8, 15, 13,  5, 38,  0, 23,  1, 19,  0, 20,  8,  5,  0,  6,  9, 18,\n",
      "         19, 20,  0, 13,  1, 14,  9,  6,  5, 19, 20,  1, 20,  9],\n",
      "        [14, 15, 18,  1, 13,  1,  0, 15,  6,  0,  1, 69,  2,  1, 20, 20, 12,  5,\n",
      "          6,  9,  5, 12,  4,  0,  1,  6, 20,  5, 18,  0, 20,  8],\n",
      "        [20,  9, 20,  9, 15, 14, 19,  0, 19,  8,  5,  0, 23,  1, 19,  0, 13,  1,\n",
      "          4,  5,  0, 15,  6, 38,  0, 15, 14,  5,  0,  8,  1,  4],\n",
      "        [12,  9, 14,  7,  0,  1,  7,  1,  9, 14, 38,  0,  8,  5, 18,  0, 14, 15,\n",
      "         18, 13,  1, 12,  0, 22,  1,  7, 21,  5, 12, 25,  0, 19],\n",
      "        [14, 20,  5, 18, 18, 21, 16, 20,  5,  4,  0,  2, 25,  0,  8,  5, 18,  0,\n",
      "          1, 14, 19, 23,  5, 18,  0, 37,  0, 62,  8,  9, 19,  0],\n",
      "        [20,  9,  3,  5,  4,  0, 20,  8,  1, 20,  0, 61, 16, 15, 20,  0, 20,  8,\n",
      "          1, 20,  0, 23,  1, 19,  0,  8,  9, 19,  0, 14,  1, 13],\n",
      "        [38,  0, 19, 15,  0, 20, 15, 69, 19, 16,  5,  1, 11, 38,  0,  1, 14,  0,\n",
      "         43, 21, 19, 20, 18,  9,  1, 14, 42,  0, 55, 15, 26,  1]],\n",
      "       dtype=torch.int32) tensor([[ 0,  4,  5,  6,  5, 14,  4,  0, 25, 15, 21, 38,  0,  6, 18,  5,  1, 11,\n",
      "          9, 19,  8, 14,  5, 19, 19, 69,  1, 14,  4,  0,  1, 12],\n",
      "        [ 0,  8,  9, 19,  0, 16,  1, 18, 20, 38,  0,  1, 14,  4,  0,  8,  5,  0,\n",
      "         18,  5,  1,  3, 20,  5,  4,  0, 20, 15,  0, 20,  8,  9],\n",
      "        [ 0,  6,  9, 18, 13,  0, 20, 15,  0, 13,  1, 18, 18, 25,  0,  1,  0, 16,\n",
      "         18, 15, 13,  9, 14,  5, 14, 20,  0, 43, 13,  5, 18,  9],\n",
      "        [20,  8,  5,  0,  4,  1, 13, 13,  5,  4,  0, 21, 16,  0, 19,  9, 12,  5,\n",
      "         14,  3,  5,  0, 15,  6,  0, 20,  8,  5,  0, 16, 18, 15],\n",
      "        [ 1, 19,  0, 14, 15, 20,  8,  9, 14,  7,  0, 23, 18, 15, 14,  7,  0, 23,\n",
      "          9, 20,  8,  0, 20,  8,  1, 20, 69,  5,  9, 20,  8,  5],\n",
      "        [19, 15, 14,  5, 18,  0, 37,  0, 61,  9,  5,  7, 13, 21, 14,  4,  0,  7,\n",
      "         15, 20,  0, 13,  5, 69,  1,  0, 12,  5, 20, 20,  5, 18],\n",
      "        [ 5, 13,  0, 21, 16, 38,  0,  1, 19,  0,  9, 19,  0,  4, 15, 14,  5,  0,\n",
      "          9, 14,  0, 20,  8,  5,  0, 13, 15, 18,  1, 12, 69,  1],\n",
      "        [ 7,  0,  1,  4, 22,  1, 14, 20,  1,  7,  5,  0,  6, 15, 18,  0,  8,  9,\n",
      "         19,  0,  3, 15, 13, 13,  9, 19, 19,  9, 15, 14,  0, 15],\n",
      "        [ 9, 11,  5,  0,  1, 12, 12,  0,  2,  1, 14, 11,  0,  4,  9, 18,  5,  3,\n",
      "         20, 15, 18, 19,  0,  2,  5,  6, 15, 18,  5,  0, 20,  8],\n",
      "        [ 8, 15, 13,  5, 38,  0, 23,  1, 19,  0, 20,  8,  5,  0,  6,  9, 18, 19,\n",
      "         20,  0, 13,  1, 14,  9,  6,  5, 19, 20,  1, 20,  9, 15],\n",
      "        [15, 18,  1, 13,  1,  0, 15,  6,  0,  1, 69,  2,  1, 20, 20, 12,  5,  6,\n",
      "          9,  5, 12,  4,  0,  1,  6, 20,  5, 18,  0, 20,  8,  5],\n",
      "        [ 9, 20,  9, 15, 14, 19,  0, 19,  8,  5,  0, 23,  1, 19,  0, 13,  1,  4,\n",
      "          5,  0, 15,  6, 38,  0, 15, 14,  5,  0,  8,  1,  4,  0],\n",
      "        [ 9, 14,  7,  0,  1,  7,  1,  9, 14, 38,  0,  8,  5, 18,  0, 14, 15, 18,\n",
      "         13,  1, 12,  0, 22,  1,  7, 21,  5, 12, 25,  0, 19,  3],\n",
      "        [20,  5, 18, 18, 21, 16, 20,  5,  4,  0,  2, 25,  0,  8,  5, 18,  0,  1,\n",
      "         14, 19, 23,  5, 18,  0, 37,  0, 62,  8,  9, 19,  0, 20],\n",
      "        [ 9,  3,  5,  4,  0, 20,  8,  1, 20,  0, 61, 16, 15, 20,  0, 20,  8,  1,\n",
      "         20,  0, 23,  1, 19,  0,  8,  9, 19,  0, 14,  1, 13,  5],\n",
      "        [ 0, 19, 15,  0, 20, 15, 69, 19, 16,  5,  1, 11, 38,  0,  1, 14,  0, 43,\n",
      "         21, 19, 20, 18,  9,  1, 14, 42,  0, 55, 15, 26,  1, 18]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(x, y)\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2syDhfkWdXt"
   },
   "source": [
    "(Question 2e: 10 points) Model definition: the basic structure should be a [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model with an [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer with input dimension `len(alphabet)` and output dimension `HIDDEN_DIM`, followed by an [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) layer with `HIDDEN_DIM` features, followed by a [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) layer with `len(alphabet)` features. Make sure that your `nn.LSTM` layer is initialized with `batch_first=True`.\n",
    "\n",
    "There are two complications here. The first is that the `nn.LSTM` layer returns a tuple of `(output, (recurrent state))`. In our model output, we only need the `output` tensor. The second is that the `nn.LSTM` layer returns the entire sequence along its second dimension. To deal with this, define a layer that handles both of these issues within the `pytorch` framework.\n",
    "\n",
    "More specifically, write a class called `extract_tensor` as a subclass of [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). This class should contain the required `__init__()` method, and a `forward()` method that extract the `output` tensor from the tuple returned by the `nn.LSTM` layer and returns this `output`. We will additionally have the `extract_tensor` layer store an attribute named `return_sequences`. If `return_sequences == True`, it should return the entire output. Otherwise, it should return `output[:,-1,:]`.\n",
    "\n",
    "We will toggle the `return_sequence` attribute to be `True` for training and `False` for prediction. This necessitates the use of an `OrderedDict` in defining our model so that our layers have names to reference (this can be a useful technique in general if you would like to manipulate individual layers of a model).\n",
    "\n",
    "In general, this type of problem is something you can often handle by making simple, self-defined layers like this.\n",
    "\n",
    "This `extract_tensor` layer should be placed between the `nn.LSTM` and `nn.Linear` layers in the `nn.Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "REnQC0MXiyAL"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "return_sequences = True\n",
    "\n",
    "class extract_tensor(nn.Module):\n",
    "    def __init__(self, return_sequences):\n",
    "        super(extract_tensor, self).__init__()\n",
    "        self.model = nn.Identity() # YOUR CODE HERE\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         tensor, _ = x\n",
    "        tensor, _ = self.model(x)# YOUR CODE HERE\n",
    "        if return_sequences:\n",
    "            return tensor\n",
    "        else:\n",
    "            return tensor[:,-1,:]\n",
    "        \n",
    "layer_ordered_dict = OrderedDict([\n",
    "    ('embedding', nn.Embedding(len(alphabet),HIDDEN_DIM)),\n",
    "    ('LSTM', nn.LSTM(HIDDEN_DIM, HIDDEN_DIM, 1, batch_first=True)),\n",
    "    ('extract', extract_tensor(return_sequences)),\n",
    "    ('linear', nn.Linear(HIDDEN_DIM, len(alphabet)))])\n",
    "\n",
    "model = nn.Sequential(layer_ordered_dict) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzp4Q7vFJXVj"
   },
   "source": [
    "(Question 2f: 2 points) If we want to use the output of the model as logits for predicting a character (which we can think of as a class), what loss should we use? Name this `criterion`. Additionally, define an optimizer to use in training. As per usual, we will recommend the use of `optim.Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "u1XFATDzieB5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # YOUR CODE HERE\n",
    "optimizer = optim.Adam(model.parameters()) # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psw5Vbpk3J3Q"
   },
   "source": [
    "(Question 2g: 8 points) Train the model!\n",
    "Note:\n",
    "\n",
    "1. We will be restricting the number of iterations rather than number of epochs. A recommended, reasonable value to start with is NUM_ITER = 8000. Use a while loop outside of the usual for loop used to iterate through the DataLoader, and make sure to break the for loop if you have exceeded the prescribed number of iterations.\n",
    "2.   Given the above parameters, this training should take about 2 minutes.  \n",
    "3.   Performance will improve but will only be mediocre.  For this exercise, that is adequate.  \n",
    "4. However, to really see what this model can do, you can (should!) set a larger number of batches above and a larger hidden state so that you can take many epochs of the data.  If you train for ~10hrs on a model with 256 hidden states (as we show in class), performance can be quite good.  This step is not required for the assignment, but please do try it on your own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "wrOmFS_eV21G",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8000 - loss: 1.4190760850906372 - avg of last 100: 1.57047738432884225\r"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "NUM_ITER = 8000\n",
    "\n",
    "i = 0\n",
    "while i < NUM_ITER:\n",
    "    ####### REMOVE ########\n",
    "#     break                 # this prevents an error if running all cells\n",
    "    ####################### before having filled in this part of the code\n",
    "    losses = []\n",
    "    for batch in train_loader:\n",
    "        if i >= NUM_ITER:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "        # YOUR CODE HERE - TO DO: zero the gradient and extract your x_batch, y_batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_logit = model(x_batch)\n",
    "        \n",
    "        y_logit = y_logit.permute(0, 2, 1) # transposing the non-batch dimensions for correct calculation\n",
    "\n",
    "        y_batch = F.one_hot(y_batch.long()) # YOUR CODE HERE: translate into one-hot encoding using F.one_hot\n",
    "        \n",
    "        y_batch = y_batch.float().permute(0, 2, 1)\n",
    "        \n",
    "        loss = criterion(y_logit, y_batch) # YOUR CODE HERE: calculate the loss\n",
    "\n",
    "        # YOUR CODE HERE: backpropagate and use the optimizer to take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if i >= 100:\n",
    "            print('iteration', i, '- loss:', loss.item(),\n",
    "                  '- avg of last 100:', sum(losses[-100:])/100, end='\\r')\n",
    "        else:     \n",
    "            print('iteration', i, '- loss:', loss.item(), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbYcgKmj1zL"
   },
   "source": [
    "Here, make sure the loss goes down as it trains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmysazCNc14W"
   },
   "source": [
    "# Section 3: Did it work? Let's see what the model learned\n",
    "\n",
    "Here, we'll write some functions to see how well the model has learned to predict text and to draw samples from the model.\n",
    "\n",
    "First, we'll give you a function to \"seed\" the model with some input text and then predict the most likely future text. It will be your job to create a variation on this function in the question below, so make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "UUaB5I6VXTdi"
   },
   "outputs": [],
   "source": [
    "def predict(seed_string, sample_length=50):\n",
    "  # Convert seed_string to int\n",
    "    current_text_ints = list(int_encode(seed_string))\n",
    "    \n",
    "\n",
    "    for i in range(sample_length):\n",
    "        # Add an empty batch dimension and convert to tensor\n",
    "        text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "        text_arr = torch.tensor(text_arr)\n",
    "\n",
    "        # set our model to return only one output instead of the sequence\n",
    "        model.extract.return_sequences = False\n",
    "\n",
    "        # Get the full sequence of predictions, remove the batch dim\n",
    "        logits = model(text_arr[-1])\n",
    "\n",
    "        # Remove the batch dimension and get the final logits\n",
    "        final_logits = logits[-1]\n",
    "\n",
    "        # Get the prediction using tf.argmax\n",
    "        pred = torch.argmax(final_logits)\n",
    "\n",
    "        # Append this to `current_text_ints`\n",
    "        current_text_ints.append(pred.numpy())\n",
    "\n",
    "    return int_decode(np.array(current_text_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel centers on the probably the probably the probably the probabl\n"
     ]
    }
   ],
   "source": [
    "test_seed = \"The novel centers on \"\n",
    "print(predict(test_seed, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "90ht1OAtegjv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel centers on the same the same the same the same the same the s\n"
     ]
    }
   ],
   "source": [
    "test_seed = \"The novel centers on \"\n",
    "print(predict(test_seed, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "1gT8Oi_KgocR"
   },
   "outputs": [],
   "source": [
    "# feel free to try your own seed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5f92OC-gORk"
   },
   "source": [
    "It seems like maybe the model learned something, but the output is a little boring. Let's make it more interesting with *randomness*!\n",
    "\n",
    "Right now, the function always picks the most likely next letter. Instead, let's sample the next letter from the model's predicted probability distribution.\n",
    "\n",
    "(Question 3a: 8 points) Fill in the blanks in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "id": "m4Oo888KiQyS"
   },
   "outputs": [],
   "source": [
    "def generate(seed_string, sample_length=50):\n",
    "  # Convert seed_string to int\n",
    "    current_text_ints = list(int_encode(seed_string))\n",
    "\n",
    "    for i in range(sample_length):\n",
    "    # Add an empty batch dimension and convert to tensor\n",
    "        text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "        text_arr = torch.tensor(text_arr)\n",
    "\n",
    "        # set our model to return only one output instead of the sequence\n",
    "        model.extract.return_sequences = False\n",
    "\n",
    "        # Get the full sequence of predictions, remove the batch dim\n",
    "        logits = model(text_arr[-1])\n",
    "\n",
    "        # Remove the batch dimension and get the final logits\n",
    "        final_logits = logits[-1]\n",
    "\n",
    "        # Normalize the final_logits to a probability distribution\n",
    "        probs = F.softmax(final_logits, dim = -1) # YOUR CODE HERE\n",
    "\n",
    "        # Call .numpy so we can use a numpy function\n",
    "        probs = probs.detach().numpy()\n",
    "\n",
    "        # Sample from the probability distribution using\n",
    "        # the function np.random.choice\n",
    "        sample = np.random.choice(a = len(alphabet), p = probs)  # YOUR CODE HERE\n",
    "\n",
    "        # Append this to `current_text_ints`\n",
    "        current_text_ints.append(sample)\n",
    "\n",
    "    return int_decode(np.array(current_text_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp07ZVlUhtpy"
   },
   "source": [
    "(Question 3b: 6 points) Test this function `generate`. Is its output different from `predict` when given the same seed? How does it differ, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel centers on ranged the mirrospless\n",
      "we all he flagmainst, which\n",
      "\n",
      "there was another reason why the theory of verbal inflation didn  t seem\n",
      "to hold water: it was only and ally really too, in the earty years much leate\n",
      "\n",
      "there was another reason why the theory of verbal inflation didn  t seem\n",
      "to hold water: it was only necessary to salt the individual units of the word\n",
      "group  redemption  with a small, innocent lack of gravity , and they\n",
      "instantly came trippingly from the tongue.  Y ou ve just saved my soul!  or\n",
      "some such; who has not said something of the sort at one time or another ,\n",
      "provided of course that it refers to nothing more than the relief after a ten \n",
      "minute wait or some equally slight inconvenience t\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(generate(test_seed))\n",
    "print()\n",
    "print(generate(text[len(text) // 2 : len(text) // 2 + 100]))\n",
    "print()\n",
    "print(text[len(text) // 2 : len(text) // 2 + 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the output is different from predict as we can observe that predict has repeated chunk of words i.e., \"the probably\",\n",
      "while generate does not. The reason is that, in predict case, we only select the most likely next letter, so every letter's next letter\n",
      "is fixed and there are only finite number of characters in our alphabet.\n"
     ]
    }
   ],
   "source": [
    "print('''Yes, the output is different from predict as we can observe that predict has repeated chunk of words i.e., \"the probably\",\n",
    "while generate does not. The reason is that, in predict case, we only select the most likely next letter, so every letter's next letter\n",
    "is fixed and there are only finite number of characters in our alphabet.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy3J9i71hnWT"
   },
   "source": [
    "(Question 3c: 2 point) Try running `generate` a few times with the same seed. Are the results the same or different? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So let us not place any particular value on the cityâ€™ s name. Like all big\n",
      "cities it was made up of irregularity , change, forward spurts, failures to keep\n",
      "step, collisions of objects and interests, punctuated by unfathomable\n",
      "silences; made up of pathways and untrodden ways, of one great rhythmic\n",
      "beat as well as the chronic discord and mutual displacement of all its\n",
      "contending rhythms.\n"
     ]
    }
   ],
   "source": [
    "print('''So let us not place any particular value on the cityâ€™ s name. Like all big\n",
    "cities it was made up of irregularity , change, forward spurts, failures to keep\n",
    "step, collisions of objects and interests, punctuated by unfathomable\n",
    "silences; made up of pathways and untrodden ways, of one great rhythmic\n",
    "beat as well as the chronic discord and mutual displacement of all its\n",
    "contending rhythms.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are different since we have a randomness in selecting next letter, so even with the same probability,\n",
      "we might choose different letter each time\n"
     ]
    }
   ],
   "source": [
    "print('''The results are different since we have a randomness in selecting next letter, so even with the same probability,\n",
    "we might choose different letter each time''')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
